{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tLoad the tokenized Paradise Lost from the Gutenberg Corpus in NLTK.* https://www.nltk.org/book/ch02.html . Stem or lemmatize the words and find counts. Select the top 20 words and create a histogram. Exclude stop words and make sure you are including words of all capitalizations in your count. If there are any meaningless “words” (“thus” and single letters, etc.) that are produced in your list or top words, alter your logic to exclude them. Specify why you chose stemming or lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import untokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', ...]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "# Lowercase words\n",
    "words_lower = [word.lower() for word in words if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "other_stop_words = [\"thou\",\"thy\",'thee',\"thus\",\"shall\",\"yet\",\"though\",\"may\",\"till\"]\n",
    "\n",
    "# Remove stopwords from words list\n",
    "words_clean = [word for word in words_lower if word not in stop_words and word not in other_stop_words]\n",
    "\n",
    "#stem the words\n",
    "words_stem = [stemmer.stem(word) for word in words_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count dictionary\n",
    "count = Counter(words_stem)\n",
    "\n",
    "# Get top 10 most common words\n",
    "top_twenty = count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heaven', 485),\n",
       " ('god', 316),\n",
       " ('earth', 228),\n",
       " ('us', 187),\n",
       " ('man', 178),\n",
       " ('first', 175),\n",
       " ('high', 159),\n",
       " ('day', 157),\n",
       " ('one', 142),\n",
       " ('power', 133),\n",
       " ('like', 132),\n",
       " ('son', 132),\n",
       " ('great', 130),\n",
       " ('far', 129),\n",
       " ('death', 127),\n",
       " ('world', 122),\n",
       " ('light', 122),\n",
       " ('good', 122),\n",
       " ('hell', 119),\n",
       " ('night', 117)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_twenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
